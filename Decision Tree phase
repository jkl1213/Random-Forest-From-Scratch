The random forest model is made of many decision trees, which make decisions collectively, as opposed to the decision tree method, which only relies on a single tree. Random forest trains a large number of decision trees independently, which results in many uncorrelated decision tree models. This can improve the accuracy score from a single decsion tree greatly as it prevents overfitting (which a single decision tree tends to do, resulting in lower testing accuracy). Unlike the single decision tree, random forest can also be used to get a 'distance matrix' of the observations as we can count how many trees classify any two observations into the same label, i.e. the more trees, the more similar these two observations are. This can be useful as we can then use the distance matrix to perform other statistical analysis, such as multidimensional scaling (MDS). In addition, random forest is a form of ensemble learning which uses bagging to prevent overfitting.

We could implement random forest by making a random forest class, which has the hyperparameters such as number of variables in a tree and bootstrap data size. The random forest class will then have a fit method that can generate bootstrap indices and pass the indices one by one to another function called creat_tree. The create tree function will then construct a decision class as done in the tutorial. This decision class will hold the the index and variable selection and generate a decision tree based on the bootstrap. The tree is stored in a list object in python. We repeat this process n times by looping in the fit method of random forest. In the end we have n distinct decision trees in the forest.

Finally, we predict new data by asking each decision tree to predict the label. We then make prediction based which label is the most popular among the trees.

```
import numpy as np
import pandas as pd
#%matplotlib notebook
%matplotlib inline
import matplotlib.pyplot as plt
from IPython.core.debugger import set_trace
np.random.seed(1234)
```
#GIVEN A NODE AND COST FUNCTION, IT LOOPS THROUGH ALL POSSIBLE SPLIT IN ALL FEATURES AND VALUES UNTIL 
#IT FINS THE BEST ONE
#Define split function
def greedy_test(node, cost_fn):
    #initialize the best parameter values
    best_cost = np.inf
    best_feature, best_value = None, None
    num_instances, num_features = node.data.shape
    #sort the features to get the test value candidates by taking the average of consecutive sorted feature values 
    data_sorted = np.sort(node.data[node.data_indices],axis=0)
    #all the "cutting points" between two sorted data points by averaging two consecutive data
    test_candidates = (data_sorted[1:] + data_sorted[:-1]) / 2.
    #print("node.data_indices:",node.data_indices)
    #print("data_sorted:",data_sorted)
    #print("data_sorted[:-1]:",data_sorted[:-1])
    #print("test_candidates:",test_candidates)
    for f in range(num_features):
        #stores the data corresponding to the f-th feature
        data_f = node.data[node.data_indices, f]
        for test in test_candidates[:,f]:
            #Split the indices using the test value of f-th feature
            left_indices = node.data_indices[data_f <= test]
            right_indices = node.data_indices[data_f > test]
            
            #we can't have a split where a child has zero element
            #if this is true over all the test features and their test values  then the function returns the best cost as infinity
            #the command 'continue' means the rest of statements are all rejected if either child has zero element
            if len(left_indices) == 0 or len(right_indices) == 0:                
                continue
            #compute the left and right cost based on the current split                                                         
            left_cost = cost_fn(node.labels[left_indices])
            
            right_cost = cost_fn(node.labels[right_indices])
            num_left, num_right = left_indices.shape[0], right_indices.shape[0]
            #get the combined cost using the weighted sum of left and right cost
            cost = (num_left * left_cost + num_right * right_cost)/num_instances
            #update only when a lower cost is encountered
            if cost < best_cost:
                best_cost = cost
                best_feature = f
                best_value = test
    return best_cost, best_feature, best_value
#Define cost function 
def cost_misclassification(labels):
    counts = np.bincount(labels) 
    class_probs = counts / np.sum(counts)
    #you could compress both the steps above by doing class_probs = np.bincount(labels) / len(labels)
    return 1 - np.max(class_probs)
#computes the gini index cost
def cost_gini_index(labels):
    class_probs = np.bincount(labels) / len(labels)
    return 1 - np.sum(np.square(class_probs)) 
#Define a node 
class Node:
    #data_indices = number of data points = data.shape[0] ; parent = None at the beginning 
    #(so if parent resutls in false)
    def __init__(self, data_indices, parent):
        self.data_indices = data_indices                    #stores the data indices which are in the region defined by this node
        self.left = None                                    #stores the left child of the node 
        self.right = None                                   #stores the right child of the node
        self.split_feature = None                           #the feature for split at this node
        self.split_value = None                             #the value of the feature for split at this node
        if parent:
            self.depth = parent.depth + 1                   #obtain the dept of the node by adding one to dept of the parent 
            self.num_classes = parent.num_classes           #copies the num classes from the parent 
            self.data = parent.data                         #copies the data from the parent
            self.labels = parent.labels                     #copies the labels from the parent
            #returns the frequency array of labels, eg. 1,1,3
            class_prob = np.bincount(self.labels[data_indices], minlength=self.num_classes) #this is counting frequency of different labels in the region defined by this node
            #returns the probability array of labels, eg.0.2 0.2 0.6
            self.class_prob = class_prob / np.sum(class_prob)  #stores the class probability for the node
            #note that we'll use the class probabilites of the leaf nodes for making predictions after the tree is built

            #Define a decision tree
class DecisionTree:
    
    #determining hyperparameters
    def __init__(self, idx, f_idx, num_classes=None, max_depth=3, cost_fn=cost_gini_index, min_leaf_instances=1):
        self.max_depth = max_depth      #maximum dept for termination 
        #empty root at the beginning
        self.root = None                #stores the root of the decision tree 
        self.cost_fn = cost_fn          #stores the cost function of the decision tree 
        self.num_classes = num_classes  #stores the total number of classes
        self.min_leaf_instances = min_leaf_instances  #minimum number of instances in a leaf for termination
        self.idx = idx
        self.f_idx = f_idx
    
    def fit(self, data, labels):
        pass                            #pass in python 3 means nothing happens and the method here is empty
    
    def predict(self, data_test):
        pass

def fit(self, data, labels):
    self.data = data
    self.labels = labels
    if self.num_classes is None:
        self.num_classes = np.max(labels) + 1
    #below are initialization of the root of the decision tree 
    #(setting up the node object for the root)
    self.root = Node(np.arange(data.shape[0]), None)
    #since the 'if parent' will be false at the beginning due to None argument above
    #we have to set up data and labels ourselves at the beginning
    self.root.data = data
    self.root.labels = labels
    self.root.num_classes = self.num_classes
    self.root.depth = 0
    #to recursively build the rest of the tree
    self._fit_tree(self.root)
    return self
def _fit_tree(self, node):
    
    #this gives condition for stopping loop later on
    #check that we do not terminate the tree development due to max depth requirement
    #This gives the condition for termination of the recursion resulting in a leaf node
    if node.depth == self.max_depth or len(node.data_indices) <= self.min_leaf_instances:
        return
    
    #greedily select the best test by minimizing the cost
    cost, split_feature, split_value = greedy_test(node, self.cost_fn)
    #if the cost returned is infinity it means that it is not possible to split the node and hence terminate
    if np.isinf(cost):
        return
    #print(f'best feature: {split_feature}, value {split_value}, cost {cost}')
    #to get a boolean array confirming data indices corresponding to this node are in the left of the split
    test = node.data[node.data_indices,split_feature] <= split_value
    #store the split feature and value of the node
    node.split_feature = split_feature
    node.split_value = split_value
    #define new nodes which are going to be the left and right child of the present node
    left = Node(node.data_indices[test], node)
    right = Node(node.data_indices[np.logical_not(test)], node)

    #recursive call to the _fit_tree()
    #the loop will stop when the condition in the first line above is reached
    self._fit_tree(left)
    self._fit_tree(right)
    #assign the left and right child to present child (node.left is also a node)
    node.left = left
    node.right = right

DecisionTree.fit = fit
DecisionTree._fit_tree = _fit_tree

def predict(self, data_test):
    class_probs = np.zeros((data_test.shape[0], self.num_classes))
    for n, x in enumerate(data_test):
        node = self.root
        #loop along the depth of the tree looking region where the present data sample fall in based on the split feature and value
        while node.left:
            #print("x: ",x)
            #print("node.split_feature: ",node.split_feature)
            #print("node.split_value: ",node.split_value)
            if x[node.split_feature] <= node.split_value:
                node = node.left
            else:
                node = node.right
        #the loop terminates when you reach a leaf of the tree and the class probability of that node is taken for prediction
        class_probs[n,:] = node.class_prob
    return class_probs

DecisionTree.predict = predict
